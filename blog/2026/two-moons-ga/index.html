<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Two Moons, Binary Weight Neural Networks, and Evolution | Jack J. Willis </title> <meta name="author" content="Jack J. Willis"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jackjameswillis.github.io/blog/2026/two-moons-ga/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jack</span> J. Willis </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Two Moons, Binary Weight Neural Networks, and Evolution</h1> <p class="post-meta"> Created on January 03, 2026 by Jack James Willis </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/tag/evolution"> <i class="fa-solid fa-hashtag fa-sm"></i> evolution</a>   <a href="/blog/tag/genetic"> <i class="fa-solid fa-hashtag fa-sm"></i> genetic</a>   <a href="/blog/tag/microbe"> <i class="fa-solid fa-hashtag fa-sm"></i> microbe</a>   <a href="/blog/tag/binary"> <i class="fa-solid fa-hashtag fa-sm"></i> binary</a>   <a href="/blog/tag/mlp"> <i class="fa-solid fa-hashtag fa-sm"></i> MLP</a>   ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a>   <a href="/blog/category/alife2026"> <i class="fa-solid fa-tag fa-sm"></i> ALIFE2026</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction">Introduction</h1> <p>Our initial step toward an ALIFE submission draws maximally on a trend amongst some of my favourite papers. Namely, that they start with demonstrations that are small, understandable, perhaps not groundbraking, nor proving the narrative in and of themselves, but justify further investigation. Admittedly, my approach thus far has been exactly the opposite, starting with SOTA MNIST classification isn’t totally insane but it isn’t efficient. Retractions in ambition since the start of 2025 lead me down to the approach taken in the first experiments on ADAM, where logistic regression on MNIST is the toy testbed (1). One of the fantastic properties of this problem is that the trained models are inherently interpretable, with weights that can be represented as images that reveal the learned features clearly. An approach that fails at this problem will, besides achieving high loss, be accompanied by weights that are visually without clear features.</p> <p>A good toy problem doesn’t demonstrate SOTA, but it permits the demonstration of some other feature of an approach. Shortest path problems like the traveling salesman problem can be used to demonstrate the power of simple distributed computational agents, such as in experiments on slime mold (2). This paper isn’t trying to prove that we should replace modern computers with slime, but aims to illuminate the key features of simple behaviours that allow natural systems to avoid modern computational paradigms. There is an amazing efficiency about properties like shortest path finding in Physarum Polycephalum, something we can draw from and look for in the world around us, and to that extent the toy problem serves well.</p> <h1 id="two-moons">Two Moons</h1> <p>In this blog we are particularly interested in toy problems that allow us to demonstrate something about distributed computation. We need a toy problem that isn’t trivial, preferably exposing approaches to nested optimization problems that are typically considered challenging from an evolutionary perspective. In this we look to a paper on the Bayesian learning rule for optimizing binary weight neural networks (3). This paper uses the two moons dataset as its toy, which is similar to an XOR problem.</p> <p>XOR TRUTH TABLE | X | Y | Z | | 0 | 0 | 0 | | 0 | 1 | 1 | | 1 | 0 | 1 | | 1 | 1 | 0 |</p> <p>XOR famously cannot be solved by a single neuron, a fact that supposedly started the AI winter. Two moons shares this fact, specifically that a straight line cannot seperate the two classes:</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/classification_challenge-480.webp 480w,/assets/img/classification_challenge-800.webp 800w,/assets/img/classification_challenge-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/classification_challenge.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>An S-shaped curve is optimal, which can only be achieved in neural computation by going deep. The Bayesian learning rule paper employs a two hidden layer MLP, each with 64 neurons, tanh activation, and binary weights to solve this problem.</p> <h1 id="binary-weight-neural-networks">Binary Weight Neural Networks</h1> <p>But why binary weights? Generally speaking, binary weights are valuable because they simplify multiplication. First, I make the admittion every good binary boy makes. We aren’t actually in binary, instead we only allow weights to take values -1 or 1. If we assume weights take on of these values, multiplication either flips the sign of the input or does nothing. This simplification of multiplication can improve runtime and memory (a weight can be stored in a single bit and projected to its sign value), but reduces how expressive neurons are. This trade-off means we tend to need more neurons to achieve performance similar to a floating point, high precision (i.e. more bits per weight) model, but not so many that speed and memory are worse (in theory… we will leave this to another blog).</p> <p>It’s not all good news though. There’s a huge problem with networks with discrete weights like binary weight networks. They are non-differentiable, meaning they cannot be trained in the standard way. The literature on training recipes that allow discrete networks to be trained is part of a field called quantization-aware training, and essentially gives up on discrete weights during training. This field is extremely dense, and I will not go into it here, but the TLDR is that quantization-aware training is at least as expensive in speed and memory as training more expressive models, but does work, and produces substantially more efficient networks when training is over.</p> <h1 id="evolution">Evolution</h1> <p>There is a space of algorithms that can train a discrete model without disregarding the discrete part. It is called combinatorial optimization, and encapsulates a vast array of algorithms also known as ‘good old fashioned AI’. One family of computational paradigms within this umbrella is the evolutionary paradigm, which has the superpower of being a distributed method. Like evolution by natural selection, where populations change over time through competition, reproduction, and heredity between individuals, evolutionary computation gives parts of the optimization process to different computers, allowing them to progress through time independently and simultaneously. This provides similar benefits to what GPUs provide for normal training methods, but importantly doesn’t rely on analytic gradients. In other words, we can train our efficient binary weight network via evolution without resorting to the GPU, if we have many CPUs. Thankfully, CPUs are cheap to rent if you don’t have a couple of thousand desktops lying around, but our toy problem is a nice size for simulating the evolutionary process on even a laptop.</p> <p>An advantage to simulation is that we can freely manipulate the topology of the selective and reproductive network. What this means is that we can choose to spread our population out on a ball, on a grid, on a line, in a small world, or we can allow them all to compete and reproduce freely with one another. As a proof of principle, we take the latter approach for this blog, and optimize a population of 10 individuals for 1000 generations.</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/full_analysis-480.webp 480w,/assets/img/full_analysis-800.webp 800w,/assets/img/full_analysis-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/full_analysis.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>The bottom right shows the output behaviour of the best individual of our population in the final generation, while the other plots show how the performance of the average performance of members of the population changes over time. We haven’t found the optimal solution, that much is clear, but it seems like we are getting close, and there’s potentially room for improvement if we allow for more generations.</p> <p>These results are honestly not great, but that is what I expect. Globally connected evolution and a small population is a recipe for bad phenotypes (i.e. output behaviour), but there is so much left to be explored here that can potentially improve our outcome.</p> <h1 id="concluding-remarks">Concluding Remarks</h1> <p>In the next blog posts we will be taking a deeper dive into the parameters of this experiment, first tackling the vast space of evolutionary operators.</p> <h1 id="bib">Bib</h1> <ul> <li>(1) Kingma, D.P., 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.</li> <li>(2) Tero, A., Takagi, S., Saigusa, T., Ito, K., Bebber, D.P., Fricker, M.D., Yumiki, K., Kobayashi, R. and Nakagaki, T., 2010. Rules for biologically inspired adaptive network design. Science, 327(5964), pp.439-442.</li> <li>(3) Meng, X., Bachmann, R. and Khan, M.E., 2020, November. Training binary neural networks using the bayesian learning rule. In International conference on machine learning (pp. 6852-6861). PMLR.</li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/second-alife/">Global Microbial Genetics Ep. 2 Decision Boundaries Varying Quantization and Activation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/first-alife/">Global Microbial Genetics Ep 1. Decision Boundaries Varying Loss and Hidden Activation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/quantized-ramble/">A Ramble on Quantized Neural Networks</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Jack J. Willis. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>