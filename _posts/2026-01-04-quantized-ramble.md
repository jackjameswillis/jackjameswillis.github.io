---
layout: post
title: "A Ramble on Quantized Neural Networks"
date: 2026-01-04
author: "Jack James Willis"
categories: [research, ramble]
tags: [binary, optimization]
summary: "Binary is not well suited to the modern AI accelerator, but INT8 is."
canonical: ""
draft: false
---

# Introduction

I planned on doing a different post today, one which requires a concentrated effort in coding and experimentation. But i've found myself extremely busy, and i'm sparing half an hour or so to cram something in. I've made it an objective to post here every day, and I won't give up just because things aren't going to plan! Thankfully I am constantly rambling to myself about research, and today i've entertained myself with rambles about quantization, which i'll document here. Almost every experiment I do is with binary genomes. This is a hold-over from the beginning of my degree, when I was completely out of my depth and had an embarrisingly poor understanding of maths. 

From 2021 to 2023 (at least, damn I was slow!) I was doing experiments on using discrete Hopfield networks and Hebbian learning for combinatorial optimization. Although it has been known for a long time, essentially since Hopfield began on the topic, that these networks could be used for combinatorial optimization, it was in the sense that we knew a way of making the dynamics of the network minimize some fitness function a-priori. My experiments, inspired by evolutionary connectionism and natural induction (no citations today i'm afraid, i've got no time!), started by imposing dynamics on the model, like the wind imposing its will on a tree, allowing the network to minimize the implicit fitness of this force before habituating to it, like how a windswept tree is permanently bent. Mathematically the only difference here between Hopfield's conception and mine is that Hopfield saw a self-enclosed system, and I saw a network as being part of a larger dynamic.

What a tangent from the point! I'll have to do a post about this work sometime, but for now i'll bring it back. My point is that the models I was interacting with used binary state vectors, and so I was familiar with this jagged-edged world of combinatorial optimization far before continuous optimization. My entry into neural networks, to the extent that I was somewhat confident in my understanding, was when I started considering the possibility of optimizing a neural network in the kind of way I performed combinatorial optimization. This was great for me since I was able to play with neural networks, but gave me the least of modern tools to help me. The issue is that modern GPU architectures are optimized for either floating point or integer linear algebra. The primary benefit of binary neural networks is in their optimal forward pass, which involves a large number of simple logical operations on bits. One can produce optimized GPU code for BNNs, but I was not (and still am not) in a position to confidently work on such software. 

Something that made matters worse was that combinatorial optimization algorithms like genetic algorithms have real bottleknecks when epistasis is a dominant feature (i.e. phenotype expressions that involve the co-expression of many genes), and neural network weights are like this on steroids. There is good reason why the best approach that we know of for learning these weights depends upon the chain rule. Consequently, when I would tell people about my work on evolving neural network parameters (not even the architectures, just the weights) I got concerned looks, like I was sick or something. Never-the-less, I had some evidence that you could optimize binary neural network this way, just not that it was optimal.

Looking back on this time makes it clear that I had a narrative problem. I knew that there was a good reason for working on this topic, in particular that binary weights meant less VRAM was required to play with larger models (my first experiments were done on a 1060 3GB, looking at 3 hidden layer 1000 dim MLPs), but it was like I was solving a problem only I had. As it turns out, I was solving a problem that anyone with a phone has, but it was not at all clear at the time that this problem existed. Thanks to small language models like Gemma 3 1B it has become feasible to control mobile devices with text (see Google edge gallery). However, even smaller models like this are essentially useless on an edge device unless quantized, but never to 1 bit, normally to INT8.

Why is this? Why not just quantize to 1 bit if its so efficient? I think a good way of understanding the problem here is to think about what high precision parameter actually is. Fundementally, we're all playing with bits. Some people organize their bits into float, others into integers, a couple like to play with the sand like me. You can think of a bit like sand, and a float like a sand castle. You ask me 'How can I make my sand castle nice and compact like your sand?' and I come over and stomp your creation to nothing. You have what you wanted at the cost of all of what you had, even though your stuff and my stuff, they're essentially the same. The trouble from my perspective, as the guy who likes sand, is that I envy your castle, but all i've got is individual grains. It seems to me like your castle contains an amount of sand i'll never be able to accumulate. The middle-ground is the fruitful place. This is where INT8 lives, which is like having a handful of wet sand. Each grain contributes to it, and a couple of handfuls can make a sand castle.

Enough of sand, and enough of this post. I'll leave this ramble with a conclusion, that my experiments are funky and cool but utterly pointless unless I can build something with them, and if I stick around with the useless atoms i'll never see an omlette. I cannot help myself...
