---
layout: post
title: "2026 â€” introduction to the New Year"
date: 2026-01-02
author: "Jack James Willis"
categories: [research, roadmap]
tags: [2026 plan, research agenda, goals, roadmap, deliverables]
summary: "2026 is the year of the doctorate. With consistency, focus, and collaboration, it can be done."
canonical: ""
draft: false
---

The first part of this post (Introduction to the New Year) is personal and possibly a bit waffly, for the meat and potatoes skip to ALIFE 2026.

# Introduction to the New Year

The last 5 years have been tumultuous, but gradually more stable, with 2025 being remarkably so. At the start of the year I began my work on Binary Neural Networks, with fragments of a narrative that lay latent, and slowly built up to the position I am at now, where my research has legs to stand on, and I can clearly see positive directions to head in. One of the hardest things i've faced throughout my degree has been a shame imposed on myself for being so slow, when the great people at the university seem to be making rapid progress. Looking back on this shame from the start of the new year I want to laugh, not at myself, but with myself, for I knew that I would hold on, keep pushing, and make it out the other end somehow, and here I am.

But here is not the end, really it is just a new beginning. The most important obstacles still lie ahead of me, in particular I must strive hard toward publishing, but beyond that I can now see that the PhD process is just time enough to prepare a person for the rest of a research career. At present, with no job on the horizon, but with freedom and access to collaboration, I am perfectly positioned to take definite steps into research, and feel an exciting energy to do so. Fostering this precious energy feels vital to me, to not let the flame dwindle, but to use the fire to work hard, to write and to produce, in research but also in my personal life and other projects.

So now I take the opportunity, in the calm before the fervor, to plan my course. I need resiliance, robustness, and purposeful direction in everything I do, and to motivate this I have decided to start my blog, and to commit to daily posts, at first for the rest of Jan, on research. And in this first post, I describe the seed for the rest, in essence the future I see in my projections and the goals and resourcefulness required to arrive there.

The acquisition of my degree is simplified by publication, is what I have believed. But I realize that the life of an academic cannot be organized around such thoughts. Publication is not a means to an end, but the end in and of itself. The commitment to a narrative, to truthful analysis, and to communication between peers for the betterment of all is what I want and believe in now. Having said this, I am not in a position to take liberty with where and what I can and cannot publish, so I focus first on this task.

# ALIFE 2026

Location: Waterloo, Ontario, Canada

Dates: August 17, 2026 - August 21, 2026

Submission Deadline: April 6, 2026

Today is the 2nd Jan, giving me 94 days (13 weeks and 3 days) until the submission deadline.

<h1>ALIFE Submission Details</h1>

My Paper: Distributed Learning of Binary Weight Neural Networks

The experimental results for this paper have not been cleanly produced, which is the primary bottlekneck for the paper. As of yet, I have clear results (although they should be substantially polished before submission) on MNIST classification of quantized logistic regression, and strong evidence on quantized multi-layer perceptrons and convolutional neural networks, but need first to clearly demonstrate results on all three, and to provide results in a format that I can present to the lab at Sussex. Previously I have used MNIST as my experimental benchmark dataset, but I have been inspired by a number of fantastic papers that look into different topics (mechanistic interpretability, neuro-evo) using much more basic datasets. One of my early inspirations was a paper on the use of the Bayesian learning rule for optimization of binary neural networks (ref 1), which uses a 2-hidden-layer binary weight MLP with hidden dim of 64 to minimize loss on the two moons classification problem. I have results, although again not clearly visualized but sufficiently so to convey the basic point, that show an evolutionary strategy is capable of performing as well as analytic gradient methods, like the straight through estimator and the Bayesian approach, in optimizing such a network. The spear of my argument does not depend on computer vision at all, but on the efficacy of distributed computation as a means of de-centralizing AI. Therefore, I should not be concerned with MNIST during these 94 days, at least until I have the heart of a paper that demonstrably proves my basic premise on two moons.


Getting feedback on my work will be vital for maximizing the probability of a successful submission to ALIFE, and having enough time after feedback to enhance my work too. Therefore, my first push is to get vital figures together before the end of Jan, and to have a brainstorm session with the lab on them and the narrative i've concocted.

Problems such as MNIST and Imagenet classification would be the natural continuation of this line of research, on which I already have good evidence of fruit. There is a great body of work to draw from on this, much I have already written about, but generally is more suited to a conference/journal specifically for neural network and quantization methods.

# Future Research

In 2025 a number of possible directions presented themselves to me, none of which I have done the justice of an investigation. But in the last few weeks I have realized a gaping whole in my narrative, which is the safety aspect of distributed AI. Consequently, more opportunities than expected appear before me. Prior topics include the viral microbial genetic algorithm and non-neural parameterized programs, but I would also like to do some interpretability work on binary neural networks, in which there is a big gap of literature. What particularly interests me is the idea of the binary neural network being essentially a large logical expression. If this expression can be extracted from the network it can replace the network, and would make for an interesting artical (i.e. Title: A Logical Expression for MNIST Classification). Essay style work would be fruitful to place in the PhD relating to safety in a distributed AI world, on which there is surely a volume of literature that I am currently oblivious to. Taking steps toward model-agnostic optimization is also a potential direction (i.e. all members of a species have similar but not identical genomes in both length and genes, and yet phenotypes are highly stable). There are likely more, but to share them would take time I do not have right now.

# Closing Remarks

Tomorrow the hard work begins, but I am honestly greatly looking forward to it and the rest of the year. This January I feel like i'm in a more stable position than I have been in many years, with a lot to look forward to and a lot of work to get done, but importantly I feel I have so many questions I want answered, and there are very few people who I think are better positioned to get answers than me.

I'll end this post with a message to the Jack of 2027. For anyone else reading this, I hope you found it interesting, and if so visit again for more juicy updates.

# Message to Jack 2027:

Remember when the Mayan calander predicted that the world would end? Remember when you heard about Kony and feared being snatched? Remember when you failed your a-levels, and had no idea what you were going to do with yourself? Scary stuff. Funny to think that doing this degree scared you at least as much from time to time. It's also funny to think of all the crazy things that have happened since you left home, all the things you've forgotten about, and the things you can't forget. It's the 2nd Jan 2026 right now, i've just turned 27, and in many ways it feels like my first day on Earth. So much happened in 2025, some of it feels like yesterday (in fact, the progression review that happened almost exactly a year ago feels like it's still happening a bit!), most has been forgotten. But I know who you are, at least at the core. I know what you care about the most, and what you cherish. I hope you still have that, no matter what has happened to you. And if you don't have it (i've been thinking about how this could happened, damn you Australia and your lure of adventure!), don't lose hope! You have so much love behind you, it's there whenever you need it, i'm sure you remember that. If all is well that's great, don't stop, keep pushing, but otherwise you should know that I have, you have, faced losing it all before, and accepted that risk, and accepted the life that lives afterwards. That's not an excuse by the way. It's no excuse to give up on everything. There's so much joy even in a life spend on a lonely rock, when your friends and family are gone, and it's just you and Him left.

"I hear Christine singing:
'O Love, that will not let me go, 
I rest my weary sould in Thee!' 
Sing, Christine, sing! Be not bitter, as Lot's wife was. Forgive them, forgive them; for they have loved much!... I wish I could live my life again. I wish I could write my story again. I have judged people. I do not want to judge people. I want to bless. I want to bless every sould who have ever lived and laughed and suffered on this whore of an island, this island in the sun, this island in God's sea!
I am on the last page of the last of my three big books. Who will ever believe I have written these three big books? I want to write another. Next time I go to Town, I will buy another from the Press. I want to write down in it all the good thoughts I have left out in this. Now it is high time I thought of going to bed. I mustn't forget to wind the clock; and I will turn the lamp down, but not right out. I don't like it in the dark. I like to be able to see my two china dogs while I am falling asleep. Damme, I am tired, me! I will sleep well tonight, I know. Ah well, that is all for now. A la prochaine!" Ebenezer Le Page.

# BIB

- (1) Meng, X., Bachmann, R. and Khan, M.E., 2020, November. Training binary neural networks using the bayesian learning rule. In International conference on machine learning (pp. 6852-6861). PMLR.
