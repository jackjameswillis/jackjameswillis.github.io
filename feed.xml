<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://jackjameswillis.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jackjameswillis.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-04T19:21:15+00:00</updated><id>https://jackjameswillis.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Two Moons, Binary Weight Neural Networks, and Evolution</title><link href="https://jackjameswillis.github.io/blog/2026/two-moons-ga/" rel="alternate" type="text/html" title="Two Moons, Binary Weight Neural Networks, and Evolution"/><published>2026-01-03T00:00:00+00:00</published><updated>2026-01-03T00:00:00+00:00</updated><id>https://jackjameswillis.github.io/blog/2026/two-moons-ga</id><content type="html" xml:base="https://jackjameswillis.github.io/blog/2026/two-moons-ga/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Our initial step toward an ALIFE submission draws maximally on a trend amongst some of my favourite papers. Namely, that they start with demonstrations that are small, understandable, perhaps not groundbraking, nor proving the narrative in and of themselves, but justify further investigation. Admittedly, my approach thus far has been exactly the opposite, starting with SOTA MNIST classification isn’t totally insane but it isn’t efficient. Retractions in ambition since the start of 2025 lead me down to the approach taken in the first experiments on ADAM, where logistic regression on MNIST is the toy testbed (1). One of the fantastic properties of this problem is that the trained models are inherently interpretable, with weights that can be represented as images that reveal the learned features clearly. An approach that fails at this problem will, besides achieving high loss, be accompanied by weights that are visually without clear features.</p> <p>A good toy problem doesn’t demonstrate SOTA, but it permits the demonstration of some other feature of an approach. Shortest path problems like the traveling salesman problem can be used to demonstrate the power of simple distributed computational agents, such as in experiments on slime mold (2). This paper isn’t trying to prove that we should replace modern computers with slime, but aims to illuminate the key features of simple behaviours that allow natural systems to avoid modern computational paradigms. There is an amazing efficiency about properties like shortest path finding in Physarum Polycephalum, something we can draw from and look for in the world around us, and to that extent the toy problem serves well.</p> <h1 id="two-moons">Two Moons</h1> <p>In this blog we are particularly interested in toy problems that allow us to demonstrate something about distributed computation. We need a toy problem that isn’t trivial, preferably exposing approaches to nested optimization problems that are typically considered challenging from an evolutionary perspective. In this we look to a paper on the Bayesian learning rule for optimizing binary weight neural networks (3). This paper uses the two moons dataset as its toy, which is similar to an XOR problem.</p> <p>XOR TRUTH TABLE | X | Y | Z | | 0 | 0 | 0 | | 0 | 1 | 1 | | 1 | 0 | 1 | | 1 | 1 | 0 |</p> <p>XOR famously cannot be solved by a single neuron, a fact that supposedly started the AI winter. Two moons shares this fact, specifically that a straight line cannot seperate the two classes:</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/classification_challenge-480.webp 480w,/assets/img/classification_challenge-800.webp 800w,/assets/img/classification_challenge-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/classification_challenge.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>An S-shaped curve is optimal, which can only be achieved in neural computation by going deep. The Bayesian learning rule paper employs a two hidden layer MLP, each with 64 neurons, tanh activation, and binary weights to solve this problem.</p> <h1 id="binary-weight-neural-networks">Binary Weight Neural Networks</h1> <p>But why binary weights? Generally speaking, binary weights are valuable because they simplify multiplication. First, I make the admittion every good binary boy makes. We aren’t actually in binary, instead we only allow weights to take values -1 or 1. If we assume weights take on of these values, multiplication either flips the sign of the input or does nothing. This simplification of multiplication can improve runtime and memory (a weight can be stored in a single bit and projected to its sign value), but reduces how expressive neurons are. This trade-off means we tend to need more neurons to achieve performance similar to a floating point, high precision (i.e. more bits per weight) model, but not so many that speed and memory are worse (in theory… we will leave this to another blog).</p> <p>It’s not all good news though. There’s a huge problem with networks with discrete weights like binary weight networks. They are non-differentiable, meaning they cannot be trained in the standard way. The literature on training recipes that allow discrete networks to be trained is part of a field called quantization-aware training, and essentially gives up on discrete weights during training. This field is extremely dense, and I will not go into it here, but the TLDR is that quantization-aware training is at least as expensive in speed and memory as training more expressive models, but does work, and produces substantially more efficient networks when training is over.</p> <h1 id="evolution">Evolution</h1> <p>There is a space of algorithms that can train a discrete model without disregarding the discrete part. It is called combinatorial optimization, and encapsulates a vast array of algorithms also known as ‘good old fashioned AI’. One family of computational paradigms within this umbrella is the evolutionary paradigm, which has the superpower of being a distributed method. Like evolution by natural selection, where populations change over time through competition, reproduction, and heredity between individuals, evolutionary computation gives parts of the optimization process to different computers, allowing them to progress through time independently and simultaneously. This provides similar benefits to what GPUs provide for normal training methods, but importantly doesn’t rely on analytic gradients. In other words, we can train our efficient binary weight network via evolution without resorting to the GPU, if we have many CPUs. Thankfully, CPUs are cheap to rent if you don’t have a couple of thousand desktops lying around, but our toy problem is a nice size for simulating the evolutionary process on even a laptop.</p> <p>An advantage to simulation is that we can freely manipulate the topology of the selective and reproductive network. What this means is that we can choose to spread our population out on a ball, on a grid, on a line, in a small world, or we can allow them all to compete and reproduce freely with one another. As a proof of principle, we take the latter approach for this blog, and optimize a population of 10 individuals for 1000 generations.</p> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/full_analysis-480.webp 480w,/assets/img/full_analysis-800.webp 800w,/assets/img/full_analysis-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/full_analysis.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>The bottom right shows the output behaviour of the best individual of our population in the final generation, while the other plots show how the performance of the average performance of members of the population changes over time. We haven’t found the optimal solution, that much is clear, but it seems like we are getting close, and there’s potentially room for improvement if we allow for more generations.</p> <p>These results are honestly not great, but that is what I expect. Globally connected evolution and a small population is a recipe for bad phenotypes (i.e. output behaviour), but there is so much left to be explored here that can potentially improve our outcome.</p> <h1 id="concluding-remarks">Concluding Remarks</h1> <p>In the next blog posts we will be taking a deeper dive into the parameters of this experiment, first tackling the vast space of evolutionary operators.</p> <h1 id="bib">Bib</h1> <ul> <li>(1) Kingma, D.P., 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.</li> <li>(2) Tero, A., Takagi, S., Saigusa, T., Ito, K., Bebber, D.P., Fricker, M.D., Yumiki, K., Kobayashi, R. and Nakagaki, T., 2010. Rules for biologically inspired adaptive network design. Science, 327(5964), pp.439-442.</li> <li>(3) Meng, X., Bachmann, R. and Khan, M.E., 2020, November. Training binary neural networks using the bayesian learning rule. In International conference on machine learning (pp. 6852-6861). PMLR.</li> </ul>]]></content><author><name>Jack James Willis</name></author><category term="research"/><category term="ALIFE2026"/><category term="evolution"/><category term="genetic"/><category term="microbe"/><category term="binary"/><category term="MLP"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">2026 — introduction to the New Year</title><link href="https://jackjameswillis.github.io/blog/2026/2026-plan-and-research-agenda/" rel="alternate" type="text/html" title="2026 — introduction to the New Year"/><published>2026-01-02T00:00:00+00:00</published><updated>2026-01-02T00:00:00+00:00</updated><id>https://jackjameswillis.github.io/blog/2026/2026-plan-and-research-agenda</id><content type="html" xml:base="https://jackjameswillis.github.io/blog/2026/2026-plan-and-research-agenda/"><![CDATA[<p>The first part of this post (Introduction to the New Year) is personal and possibly a bit waffly, for the meat and potatoes skip to ALIFE 2026.</p> <h1 id="introduction-to-the-new-year">Introduction to the New Year</h1> <p>The last 5 years have been tumultuous, but gradually more stable, with 2025 being remarkably so. At the start of the year I began my work on Binary Neural Networks, with fragments of a narrative that lay latent, and slowly built up to the position I am at now, where my research has legs to stand on, and I can clearly see positive directions to head in. One of the hardest things i’ve faced throughout my degree has been a shame imposed on myself for being so slow, when the great people at the university seem to be making rapid progress. Looking back on this shame from the start of the new year I want to laugh, not at myself, but with myself, for I knew that I would hold on, keep pushing, and make it out the other end somehow, and here I am.</p> <p>But here is not the end, really it is just a new beginning. The most important obstacles still lie ahead of me, in particular I must strive hard toward publishing, but beyond that I can now see that the PhD process is just time enough to prepare a person for the rest of a research career. At present, with no job on the horizon, but with freedom and access to collaboration, I am perfectly positioned to take definite steps into research, and feel an exciting energy to do so. Fostering this precious energy feels vital to me, to not let the flame dwindle, but to use the fire to work hard, to write and to produce, in research but also in my personal life and other projects.</p> <p>So now I take the opportunity, in the calm before the fervor, to plan my course. I need resiliance, robustness, and purposeful direction in everything I do, and to motivate this I have decided to start my blog, and to commit to daily posts, at first for the rest of Jan, on research. And in this first post, I describe the seed for the rest, in essence the future I see in my projections and the goals and resourcefulness required to arrive there.</p> <p>The acquisition of my degree is simplified by publication, is what I have believed. But I realize that the life of an academic cannot be organized around such thoughts. Publication is not a means to an end, but the end in and of itself. The commitment to a narrative, to truthful analysis, and to communication between peers for the betterment of all is what I want and believe in now. Having said this, I am not in a position to take liberty with where and what I can and cannot publish, so I focus first on this task.</p> <h1 id="alife-2026">ALIFE 2026</h1> <p>Location: Waterloo, Ontario, Canada</p> <p>Dates: August 17, 2026 - August 21, 2026</p> <p>Submission Deadline: April 6, 2026</p> <p>Today is the 2nd Jan, giving me 94 days (13 weeks and 3 days) until the submission deadline.</p> <h1>ALIFE Submission Details</h1> <p>My Paper: Distributed Learning of Binary Weight Neural Networks</p> <p>The experimental results for this paper have not been cleanly produced, which is the primary bottlekneck for the paper. As of yet, I have clear results (although they should be substantially polished before submission) on MNIST classification of quantized logistic regression, and strong evidence on quantized multi-layer perceptrons and convolutional neural networks, but need first to clearly demonstrate results on all three, and to provide results in a format that I can present to the lab at Sussex. Previously I have used MNIST as my experimental benchmark dataset, but I have been inspired by a number of fantastic papers that look into different topics (mechanistic interpretability, neuro-evo) using much more basic datasets. One of my early inspirations was a paper on the use of the Bayesian learning rule for optimization of binary neural networks (ref 1), which uses a 2-hidden-layer binary weight MLP with hidden dim of 64 to minimize loss on the two moons classification problem. I have results, although again not clearly visualized but sufficiently so to convey the basic point, that show an evolutionary strategy is capable of performing as well as analytic gradient methods, like the straight through estimator and the Bayesian approach, in optimizing such a network. The spear of my argument does not depend on computer vision at all, but on the efficacy of distributed computation as a means of de-centralizing AI. Therefore, I should not be concerned with MNIST during these 94 days, at least until I have the heart of a paper that demonstrably proves my basic premise on two moons.</p> <p>Getting feedback on my work will be vital for maximizing the probability of a successful submission to ALIFE, and having enough time after feedback to enhance my work too. Therefore, my first push is to get vital figures together before the end of Jan, and to have a brainstorm session with the lab on them and the narrative i’ve concocted.</p> <p>Problems such as MNIST and Imagenet classification would be the natural continuation of this line of research, on which I already have good evidence of fruit. There is a great body of work to draw from on this, much I have already written about, but generally is more suited to a conference/journal specifically for neural network and quantization methods.</p> <h1 id="future-research">Future Research</h1> <p>In 2025 a number of possible directions presented themselves to me, none of which I have done the justice of an investigation. But in the last few weeks I have realized a gaping whole in my narrative, which is the safety aspect of distributed AI. Consequently, more opportunities than expected appear before me. Prior topics include the viral microbial genetic algorithm and non-neural parameterized programs, but I would also like to do some interpretability work on binary neural networks, in which there is a big gap of literature. What particularly interests me is the idea of the binary neural network being essentially a large logical expression. If this expression can be extracted from the network it can replace the network, and would make for an interesting artical (i.e. Title: A Logical Expression for MNIST Classification). Essay style work would be fruitful to place in the PhD relating to safety in a distributed AI world, on which there is surely a volume of literature that I am currently oblivious to. Taking steps toward model-agnostic optimization is also a potential direction (i.e. all members of a species have similar but not identical genomes in both length and genes, and yet phenotypes are highly stable). There are likely more, but to share them would take time I do not have right now.</p> <h1 id="closing-remarks">Closing Remarks</h1> <p>Tomorrow the hard work begins, but I am honestly greatly looking forward to it and the rest of the year. This January I feel like i’m in a more stable position than I have been in many years, with a lot to look forward to and a lot of work to get done, but importantly I feel I have so many questions I want answered, and there are very few people who I think are better positioned to get answers than me.</p> <p>I’ll end this post with a message to the Jack of 2027. For anyone else reading this, I hope you found it interesting, and if so visit again for more juicy updates.</p> <h1 id="message-to-jack-2027">Message to Jack 2027:</h1> <p>Remember when the Mayan calander predicted that the world would end? Remember when you heard about Kony and feared being snatched? Remember when you failed your a-levels, and had no idea what you were going to do with yourself? Scary stuff. Funny to think that doing this degree scared you at least as much from time to time. It’s also funny to think of all the crazy things that have happened since you left home, all the things you’ve forgotten about, and the things you can’t forget. It’s the 2nd Jan 2026 right now, i’ve just turned 27, and in many ways it feels like my first day on Earth. So much happened in 2025, some of it feels like yesterday (in fact, the progression review that happened almost exactly a year ago feels like it’s still happening a bit!), most has been forgotten. But I know who you are, at least at the core. I know what you care about the most, and what you cherish. I hope you still have that, no matter what has happened to you. And if you don’t have it (i’ve been thinking about how this could happened, damn you Australia and your lure of adventure!), don’t lose hope! You have so much love behind you, it’s there whenever you need it, i’m sure you remember that. If all is well that’s great, don’t stop, keep pushing, but otherwise you should know that I have, you have, faced losing it all before, and accepted that risk, and accepted the life that lives afterwards. That’s not an excuse by the way. It’s no excuse to give up on everything. There’s so much joy even in a life spend on a lonely rock, when your friends and family are gone, and it’s just you and Him left.</p> <p>“I hear Christine singing: ‘O Love, that will not let me go, I rest my weary sould in Thee!’ Sing, Christine, sing! Be not bitter, as Lot’s wife was. Forgive them, forgive them; for they have loved much!… I wish I could live my life again. I wish I could write my story again. I have judged people. I do not want to judge people. I want to bless. I want to bless every soul who have ever lived and laughed and suffered on this whore of an island, this island in the sun, this island in God’s sea! I am on the last page of the last of my three big books. Who will ever believe I have written these three big books? I want to write another. Next time I go to Town, I will buy another from the Press. I want to write down in it all the good thoughts I have left out in this. Now it is high time I thought of going to bed. I mustn’t forget to wind the clock; and I will turn the lamp down, but not right out. I don’t like it in the dark. I like to be able to see my two china dogs while I am falling asleep. Damme, I am tired, me! I will sleep well tonight, I know. Ah well, that is all for now. A la prochaine!” Ebenezer Le Page.</p> <h1 id="bib">BIB</h1> <ul> <li>(1) Meng, X., Bachmann, R. and Khan, M.E., 2020, November. Training binary neural networks using the bayesian learning rule. In International conference on machine learning (pp. 6852-6861). PMLR.</li> </ul>]]></content><author><name>Jack James Willis</name></author><category term="research"/><category term="roadmap"/><category term="2026 plan"/><category term="research agenda"/><category term="goals"/><category term="roadmap"/><category term="deliverables"/><summary type="html"><![CDATA[The first part of this post (Introduction to the New Year) is personal and possibly a bit waffly, for the meat and potatoes skip to ALIFE 2026.]]></summary></entry><entry><title type="html">A Ramble on Quantized Neural Networks</title><link href="https://jackjameswillis.github.io/blog/2026/quantized-ramble/" rel="alternate" type="text/html" title="A Ramble on Quantized Neural Networks"/><published>2026-01-02T00:00:00+00:00</published><updated>2026-01-02T00:00:00+00:00</updated><id>https://jackjameswillis.github.io/blog/2026/quantized-ramble</id><content type="html" xml:base="https://jackjameswillis.github.io/blog/2026/quantized-ramble/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>I planned on doing a different post today, one which requires a concentrated effort in coding and experimentation. But i’ve found myself extremely busy, and i’m sparing half an hour or so to cram something in. I’ve made it an objective to post here every day, and I won’t give up just because things aren’t going to plan! Thankfully I am constantly rambling to myself about research, and today i’ve entertained myself with rambles about quantization, which i’ll document here. Almost every experiment I do is with binary genomes. This is a hold-over from the beginning of my degree, when I was completely out of my depth and had an embarrisingly poor understanding of maths.</p> <p>From 2021 to 2023 (at least, damn I was slow!) I was doing experiments on using discrete Hopfield networks and Hebbian learning for combinatorial optimization. Although it has been known for a long time, essentially since Hopfield began on the topic, that these networks could be used for combinatorial optimization, it was in the sense that we knew a way of making the dynamics of the network minimize some fitness function a-priori. My experiments, inspired by evolutionary connectionism and natural induction (no citations today i’m afraid, i’ve got no time!), started by imposing dynamics on the model, like the wind imposing its will on a tree, allowing the network to minimize the implicit fitness of this force before habituating to it, like how a windswept tree is permanently bent. Mathematically the only difference here between Hopfield’s conception and mine is that Hopfield saw a self-enclosed system, and I saw a network as being part of a larger dynamic.</p> <p>What a tangent from the point! I’ll have to do a post about this work sometime, but for now i’ll bring it back. My point is that the models I was interacting with used binary state vectors, and so I was familiar with this jagged-edged world of combinatorial optimization far before continuous optimization. My entry into neural networks, to the extent that I was somewhat confident in my understanding, was when I started considering the possibility of optimizing a neural network in the kind of way I performed combinatorial optimization. This was great for me since I was able to play with neural networks, but gave me the least of modern tools to help me. The issue is that modern GPU architectures are optimized for either floating point or integer linear algebra. The primary benefit of binary neural networks is in their optimal forward pass, which involves a large number of simple logical operations on bits. One can produce optimized GPU code for BNNs, but I was not (and still am not) in a position to confidently work on such software.</p> <p>Something that made matters worse was that combinatorial optimization algorithms like genetic algorithms have real bottleknecks when epistasis is a dominant feature (i.e. phenotype expressions that involve the co-expression of many genes), and neural network weights are like this on steroids. There is good reason why the best approach that we know of for learning these weights depends upon the chain rule. Consequently, when I would tell people about my work on evolving neural network parameters (not even the architectures, just the weights) I got concerned looks, like I was sick or something. Never-the-less, I had some evidence that you could optimize binary neural network this way, just not that it was optimal.</p> <p>Looking back on this time makes it clear that I had a narrative problem. I knew that there was a good reason for working on this topic, in particular that binary weights meant less VRAM was required to play with larger models (my first experiments were done on a 1060 3GB, looking at 3 hidden layer 1000 dim MLPs), but it was like I was solving a problem only I had. As it turns out, I was solving a problem that anyone with a phone has, but it was not at all clear at the time that this problem existed. Thanks to small language models like Gemma 3 1B it has become feasible to control mobile devices with text (see Google edge gallery). However, even smaller models like this are essentially useless on an edge device unless quantized, but never to 1 bit, normally to INT8.</p> <p>Why is this? Why not just quantize to 1 bit if its so efficient? I think a good way of understanding the problem here is to think about what high precision parameter actually is. Fundementally, we’re all playing with bits. Some people organize their bits into float, others into integers, a couple like to play with the sand like me. You can think of a bit like sand, and a float like a sand castle. You ask me ‘How can I make my sand castle nice and compact like your sand?’ and I come over and stomp your creation to nothing. You have what you wanted at the cost of all of what you had, even though your stuff and my stuff, they’re essentially the same. The trouble from my perspective, as the guy who likes sand, is that I envy your castle, but all i’ve got is individual grains. It seems to me like your castle contains an amount of sand i’ll never be able to accumulate. The middle-ground is the fruitful place. This is where INT8 lives, which is like having a handful of wet sand. Each grain contributes to it, and a couple of handfuls can make a sand castle.</p> <p>Enough of sand, and enough of this post. I’ll leave this ramble with a conclusion, that my experiments are funky and cool but utterly pointless unless I can build something with them, and if I stick around with the useless atoms i’ll never see an omlette. I cannot help myself…</p>]]></content><author><name>Jack James Willis</name></author><category term="research"/><category term="ramble"/><category term="binary"/><category term="optimization"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">a post with plotly.js</title><link href="https://jackjameswillis.github.io/blog/2025/plotly/" rel="alternate" type="text/html" title="a post with plotly.js"/><published>2025-03-26T14:24:00+00:00</published><updated>2025-03-26T14:24:00+00:00</updated><id>https://jackjameswillis.github.io/blog/2025/plotly</id><content type="html" xml:base="https://jackjameswillis.github.io/blog/2025/plotly/"><![CDATA[<p>This is an example post with some <a href="https://plotly.com/javascript/">plotly</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">plotly
</span><span class="sb">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "type": "scatter"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [16, 5, 11, 9],
      "type": "scatter"
    }
  ]
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-plotly">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "type": "scatter"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [16, 5, 11, 9],
      "type": "scatter"
    }
  ]
}
</code></pre> <p>Also another example chart.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">plotly
</span><span class="sb">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "mode": "markers"
    },
    {
      "x": [2, 3, 4, 5],
      "y": [16, 5, 11, 9],
      "mode": "lines"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [12, 9, 15, 12],
      "mode": "lines+markers"
    }
  ],
  "layout": {
    "title": {
      "text": "Line and Scatter Plot"
    }
  }
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>This is how it looks like:</p> <pre><code class="language-plotly">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "mode": "markers"
    },
    {
      "x": [2, 3, 4, 5],
      "y": [16, 5, 11, 9],
      "mode": "lines"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [12, 9, 15, 12],
      "mode": "lines+markers"
    }
  ],
  "layout": {
    "title": {
      "text": "Line and Scatter Plot"
    }
  }
}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="charts"/><summary type="html"><![CDATA[this is what included plotly.js code could look like]]></summary></entry><entry><title type="html">a post with image galleries</title><link href="https://jackjameswillis.github.io/blog/2024/photo-gallery/" rel="alternate" type="text/html" title="a post with image galleries"/><published>2024-12-04T01:59:00+00:00</published><updated>2024-12-04T01:59:00+00:00</updated><id>https://jackjameswillis.github.io/blog/2024/photo-gallery</id><content type="html" xml:base="https://jackjameswillis.github.io/blog/2024/photo-gallery/"><![CDATA[<p>The images in this post are all zoomable, arranged into different mini-galleries using different libraries.</p> <h2 id="lightbox2"><a href="https://lokeshdhakar.com/projects/lightbox2/">Lightbox2</a></h2> <p><a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p> <hr/> <h2 id="photoswipe"><a href="https://photoswipe.com/">PhotoSwipe</a></h2> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--getting-started"> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-pswp-width="1669" data-pswp-height="2500" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg" alt=""/> </a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-2500.jpg" data-pswp-width="1875" data-pswp-height="2500" data-cropped="true" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-200.jpg" alt=""/> </a> <a href="https://unsplash.com" data-pswp-src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1666" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg" alt=""/> </a> <div> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1667" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg" alt=""/> </a> </div> </div> <hr/> <h2 id="spotlight-js"><a href="https://nextapps-de.github.io/spotlight/">Spotlight JS</a></h2> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/> </a> </div> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg"/> </a> </div> <hr/> <h2 id="venobox"><a href="https://veno.es/venobox/">Venobox</a></h2> <p><a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what included image galleries could look like]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://jackjameswillis.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://jackjameswillis.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://jackjameswillis.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>May 14, 2024 We’re introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we’re introducing Gemini 1.5 Flash: a model that’s lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We’re also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5’s 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It’s optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it’s a lighter weight model than 1.5 Pro, it’s highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it’s been trained by 1.5 Pro through a process called “distillation,” where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash’s availability and pricing.Over the last few months, we’ve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we’ve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We’ve improved control over the model’s responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we’ve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we’re now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do — not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we’re also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We’re announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we’ve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind’s mission to build AI responsibly to benefit humanity, we’ve always wanted to develop universal AI agents that can be helpful in everyday life. That’s why today, we’re sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do — and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we’ve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we’ve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we’ve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they’re being used in, and respond quickly, in conversation.With technology like this, it’s easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We’ve made incredible progress so far with our family of Gemini models, and we’re always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we’re able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google’s privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let’s stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">a post with tabs</title><link href="https://jackjameswillis.github.io/blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://jackjameswillis.github.io/blog/2024/tabs</id><content type="html" xml:base="https://jackjameswillis.github.io/blog/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="17910b2d-bb35-489c-a22d-916ec6cd6656" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="17910b2d-bb35-489c-a22d-916ec6cd6656" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="53fef092-65ec-4df3-a6b0-b9460097ba7a" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="53fef092-65ec-4df3-a6b0-b9460097ba7a" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="fc8791df-5587-4a42-89b3-da83500ee6fc" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="fc8791df-5587-4a42-89b3-da83500ee6fc" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">a post with typograms</title><link href="https://jackjameswillis.github.io/blog/2024/typograms/" rel="alternate" type="text/html" title="a post with typograms"/><published>2024-04-29T23:36:10+00:00</published><updated>2024-04-29T23:36:10+00:00</updated><id>https://jackjameswillis.github.io/blog/2024/typograms</id><content type="html" xml:base="https://jackjameswillis.github.io/blog/2024/typograms/"><![CDATA[<p>This is an example post with some <a href="https://github.com/google/typograms/">typograms</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">+----+
|    |---&gt; My first diagram!
+----+</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-typograms">+----+
|    |---&gt; My first diagram!
+----+
</code></pre> <p>Another example:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.</span>
<span class="p">```</span>
</code></pre></div></div> <p>which generates:</p> <pre><code class="language-typograms">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.
</code></pre> <p>For more examples, check out the <a href="https://google.github.io/typograms/#examples">typograms documentation</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="diagrams"/><summary type="html"><![CDATA[this is what included typograms code could look like]]></summary></entry><entry><title type="html">a post that can be cited</title><link href="https://jackjameswillis.github.io/blog/2024/post-citation/" rel="alternate" type="text/html" title="a post that can be cited"/><published>2024-04-28T15:06:00+00:00</published><updated>2024-04-28T15:06:00+00:00</updated><id>https://jackjameswillis.github.io/blog/2024/post-citation</id><content type="html" xml:base="https://jackjameswillis.github.io/blog/2024/post-citation/"><![CDATA[<p>This is an example post that can be cited. The content of the post ends here, while the citation information is automatically provided below. The only thing needed is for you to set the <code class="language-plaintext highlighter-rouge">citation</code> key in the front matter to <code class="language-plaintext highlighter-rouge">true</code>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="citation"/><summary type="html"><![CDATA[this is what a post that can be cited looks like]]></summary></entry><entry><title type="html">a post with pseudo code</title><link href="https://jackjameswillis.github.io/blog/2024/pseudocode/" rel="alternate" type="text/html" title="a post with pseudo code"/><published>2024-04-15T00:01:00+00:00</published><updated>2024-04-15T00:01:00+00:00</updated><id>https://jackjameswillis.github.io/blog/2024/pseudocode</id><content type="html" xml:base="https://jackjameswillis.github.io/blog/2024/pseudocode/"><![CDATA[<p>This is an example post with some pseudo code rendered by <a href="https://github.com/SaswatPadhi/pseudocode.js">pseudocode</a>. The example presented here is the same as the one in the <a href="https://saswat.padhi.me/pseudocode.js/">pseudocode.js</a> documentation, with only one simple but important change: everytime you would use <code class="language-plaintext highlighter-rouge">$</code>, you should use <code class="language-plaintext highlighter-rouge">$$</code> instead. Also, note that the <code class="language-plaintext highlighter-rouge">pseudocode</code> key in the front matter is set to <code class="language-plaintext highlighter-rouge">true</code> to enable the rendering of pseudo code. As an example, using this code:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">pseudocode
</span><span class="sb">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Generates:</p> <pre><code class="language-pseudocode">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included pseudo code could look like]]></summary></entry></feed>